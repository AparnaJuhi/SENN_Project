{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SENN_isear.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNXUfACk+4iLdGEVib4hjzh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AparnaJuhi/SENN_Project/blob/main/SENN_isear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e6rsA5Nn2Xr",
        "outputId": "59ce0022-13f7-45fe-ce36-a93b5c9e5b5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Dense,Dropout,LSTM,Input,Bidirectional\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from ast import literal_eval\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_PJZ5fz1Tl3"
      },
      "source": [
        "Remove the rows which contains values ['No resposne']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJH_xHURn5xd",
        "outputId": "5dab5b08-6649-4755-9728-5b847272e934",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('isear.csv',header=None)\n",
        "print(len(df))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7652\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLhSlGBaz-dF",
        "outputId": "bd308e04-e336-46d0-ea96-a40ff9a70766",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.drop(df[df[1] == '[ No response.]'].index, inplace = True)\n",
        "print(len(df))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HowmuSwqpYdd"
      },
      "source": [
        "It will store the sentences that people has said which demonstrate their feelings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdBKrD2opN9J",
        "outputId": "d622f0fd-396d-4b45-91ce-c143a1e2c1ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "feel_arr=df[1]\n",
        "feel_arr[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[ On days when I feel close to my partner and other friends.  \\nWhen I feel at peace with myself and also experience a close \\ncontact with people whom I regard greatly.]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2JqD4y4qgn_"
      },
      "source": [
        "Now, Each sentence in fee_arr has been tokenised into words. Like \"hi i am here\" is converted to [\"hi\",\"i\",\"am\",\"here\"] and now stored in feel_arr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE031wSFqUWZ",
        "outputId": "25f4fe78-5449-4ca1-c857-763fb18249c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "feel_arr=[word_tokenize(sent) for sent in feel_arr]\n",
        "print(feel_arr[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[', 'On', 'days', 'when', 'I', 'feel', 'close', 'to', 'my', 'partner', 'and', 'other', 'friends', '.', 'When', 'I', 'feel', 'at', 'peace', 'with', 'myself', 'and', 'also', 'experience', 'a', 'close', 'contact', 'with', 'people', 'whom', 'I', 'regard', 'greatly', '.', ']']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0vtAFNQprkh"
      },
      "source": [
        "As the length of sentence vary, but input to LSTM will be a fixed length of sentence, so here I have chosed 100 words as my sentence length. So when, the sentence will contains more than 100 words, padd function will discard the words after 100th position. If the sentence will have less than 100 words, so rest of the array will be filled with 'pad' word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrpIWNPfpl-n"
      },
      "source": [
        "def padd(arr):\n",
        "    for i in range(100-len(arr)):\n",
        "        arr.append('<pad>')\n",
        "    return arr[:100]\n",
        "for i in range(len(feel_arr)):\n",
        "  feel_arr[i]=padd(feel_arr[i])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRj9AxW6rR9h"
      },
      "source": [
        "Initialising glove array which contains a large amounts of words and their corresponding vector representaion in 50 dimenisons.For each word in feel_arr we will have a glove vector of 50 dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9rHBx9Fq2aT",
        "outputId": "63c22f57-9e98-4d62-9001-93bac36e514a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab_f=open('glove.6B.50d.txt')\n",
        "word_to_emb={}\n",
        "for line in vocab_f:\n",
        "  word_to_emb[line.split()[0]]=[float(i) for i in line.split()[1:]] \n",
        "word_to_emb['<pad>']=[0]*50\n",
        "print(len(word_to_emb['happy']))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEcTRjK0sWeq",
        "outputId": "f63454f9-150b-4a24-e574-d7226f45b8e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "embedded_feel_arr=[] \n",
        "for each_sentence in feel_arr:\n",
        "    embedded_feel_arr.append([])\n",
        "    for word in each_sentence:\n",
        "        if word.lower() in word_to_emb:\n",
        "            embedded_feel_arr[-1].append(word_to_emb[word.lower()])\n",
        "        else:\n",
        "            embedded_feel_arr[-1].append([0]*50)\n",
        "print(embedded_feel_arr[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.61201, 0.98226, 0.11539, 0.014623, 0.23873, -0.067035, 0.30632, -0.64742, -0.38517, -0.03691, 0.094788, 0.57631, -0.091557, -0.54825, 0.25255, -0.14759, 0.13023, 0.21658, -0.30623, 0.30028, -0.23471, -0.17927, 0.9518, 0.54258, 0.31172, -0.51038, -0.65223, -0.48858, 0.13486, -0.40132, 2.493, -0.38777, -0.26456, -0.49414, -0.3871, -0.20983, 0.82941, -0.46253, 0.39549, 0.014881, 0.79485, -0.79958, -0.16243, 0.013862, -0.53536, 0.52536, 0.019818, -0.16353, 0.30649, 0.81745], [0.30045, 0.25006, -0.16692, 0.1923, 0.026921, -0.079486, -0.91383, -0.1974, -0.053413, -0.40846, -0.26844, -0.28212, -0.5, 0.1221, 0.3903, 0.17797, -0.4429, -0.40478, -0.9505, -0.16897, 0.77793, 0.33525, 0.3346, -0.1754, -0.12017, -1.7861, 0.29241, 0.55933, 0.029982, -0.32417, 3.9297, 0.1088, -0.57335, -0.17842, 0.0041748, -0.16309, 0.45077, -0.16123, -0.17311, -0.087889, -0.089032, 0.062001, -0.19946, -0.38863, -0.18232, 0.060751, 0.098603, -0.07131, 0.23052, -0.51939], [0.62236, 0.1973, 0.0023326, -0.44924, 0.084905, -0.33628, -0.90642, 0.62154, -0.20657, -0.20043, -0.6269, -0.64429, -0.31616, 0.23314, 1.2348, -0.080113, -0.7382, -0.3497, -0.97589, -0.046964, 0.29777, 0.68288, 0.87876, -0.28921, 0.1456, -1.411, -0.11301, 0.2132, 0.49548, 0.38685, 3.397, 0.40639, -0.54343, -0.00037414, 0.44268, -0.1015, 0.37693, 0.047623, 0.044414, 0.10038, -0.99794, 0.2082, 0.063226, 0.12436, -0.059387, 0.046682, -0.22335, -0.22082, -0.40414, -0.18502], [0.27062, -0.36596, 0.097193, -0.50708, 0.37375, 0.16736, -0.94185, 0.54004, -0.66669, -0.24236, 0.25876, 0.28084, -0.86643, -0.068961, 0.90346, 0.40877, -0.39563, -0.25604, -1.0316, -0.26669, -0.080584, 0.40841, 0.55885, -0.18299, 0.46494, -2.2671, 0.14102, 0.19841, 0.5153, -0.27608, 3.3604, 0.15123, -0.36693, -0.28804, 0.076042, -0.076662, 0.21897, 0.39001, 0.38684, -0.16961, -0.33674, 0.37094, -0.45911, 0.00066285, -0.17797, 0.12467, -0.015418, -0.75256, -0.17335, -0.22587], [0.11891, 0.15255, -0.082073, -0.74144, 0.75917, -0.48328, -0.31009, 0.51476, -0.98708, 0.00061757, -0.15043, 0.8377, -1.0797, -0.5146, 1.3188, 0.62007, 0.13779, 0.47108, -0.072874, -0.72675, -0.74116, 0.75263, 0.8818, 0.29561, 1.3548, -2.5701, -1.3523, 0.4588, 1.0068, -1.1856, 3.4737, 0.77898, -0.72929, 0.25102, -0.26156, -0.34684, 0.55841, 0.75098, 0.4983, -0.26823, -0.0027443, -0.018298, -0.28096, 0.55318, 0.037706, 0.18555, -0.15025, -0.57512, -0.26671, 0.92121], [-0.0044021, -0.34141, -0.22729, -0.69283, 0.80244, -0.21823, -0.19036, 0.86436, -0.6988, 0.58897, 0.22517, 0.1399, -0.18051, 0.14046, 0.55502, 0.59184, 0.51439, 0.30326, 0.37276, -1.0549, -1.0332, 1.2613, 0.20113, 0.50856, 1.0459, -1.509, -1.1532, 0.63151, 1.1575, -0.52315, 3.1025, 1.0674, 0.47125, -0.52927, -0.51614, -0.31387, 0.011424, 0.017461, 0.026483, -0.81318, -0.42375, -0.23732, 0.24397, 0.74946, 0.056762, 0.26774, -0.82833, -0.25263, -0.29477, 0.611], [0.39621, 0.15996, 0.46712, 0.18459, 1.1826, -0.29844, -0.44587, 0.13879, -0.79038, -0.21838, -0.33809, -0.68331, -0.37921, -0.11752, 0.042039, 0.48018, -0.012653, -0.45736, -0.77084, -0.047438, 0.31252, 0.5868, 0.11256, -0.25544, 0.13935, -1.3885, 0.66674, 0.28422, 0.14652, 0.21721, 3.2838, -0.064245, 0.087545, 0.059236, 0.18729, -0.7151, -0.45676, 0.42607, -0.075389, -0.21992, -0.12417, 0.12843, 0.33174, 0.089882, -0.15063, -0.08294, -0.066774, -0.19275, 0.24453, -0.28937], [0.68047, -0.039263, 0.30186, -0.17792, 0.42962, 0.032246, -0.41376, 0.13228, -0.29847, -0.085253, 0.17118, 0.22419, -0.10046, -0.43653, 0.33418, 0.67846, 0.057204, -0.34448, -0.42785, -0.43275, 0.55963, 0.10032, 0.18677, -0.26854, 0.037334, -2.0932, 0.22171, -0.39868, 0.20912, -0.55725, 3.8826, 0.47466, -0.95658, -0.37788, 0.20869, -0.32752, 0.12751, 0.088359, 0.16351, -0.21634, -0.094375, 0.018324, 0.21048, -0.03088, -0.19722, 0.082279, -0.09434, -0.073297, -0.064699, -0.26044], [-0.27279, 0.77515, -0.10181, -0.9166, 0.90477, -0.070501, -0.47569, 0.44608, 0.1697, 0.072352, -0.16306, 0.86852, -0.76634, -0.016103, 0.78492, 0.2952, -0.74859, 0.2099, 0.65537, -0.62334, -0.43711, 1.1854, 0.47519, 0.0093866, 1.1377, -2.4394, -1.5619, 0.49001, 1.0985, -0.97371, 3.4628, 1.0408, -0.65138, 0.57189, -0.12523, 0.26705, 0.16373, 0.41105, 0.7509, -0.77923, 0.03638, -0.28609, -0.72365, 0.63511, 0.089441, -0.30133, 0.36518, -0.73367, 0.040383, 0.26657], [0.16379, 0.41112, -0.13158, 0.94895, 0.70715, 1.0944, -0.68831, 0.17241, -0.095856, -0.067607, 0.41669, 0.63432, -1.3402, 0.42068, -0.12637, -0.057558, 0.76176, 0.31818, 0.46097, -0.11251, 0.37254, 0.83932, -0.40458, 0.36498, -0.30611, -1.2432, 0.57756, -0.96071, -0.57665, 0.14119, 2.4005, 0.14858, 0.065053, 0.20795, -0.079198, -0.50273, -0.82832, 0.64008, 0.1735, -0.65171, 0.33241, 0.0098905, 0.058787, -0.279, 0.21618, -0.35415, -0.76466, -0.28958, -0.38235, 0.82379], [0.26818, 0.14346, -0.27877, 0.016257, 0.11384, 0.69923, -0.51332, -0.47368, -0.33075, -0.13834, 0.2702, 0.30938, -0.45012, -0.4127, -0.09932, 0.038085, 0.029749, 0.10076, -0.25058, -0.51818, 0.34558, 0.44922, 0.48791, -0.080866, -0.10121, -1.3777, -0.10866, -0.23201, 0.012839, -0.46508, 3.8463, 0.31362, 0.13643, -0.52244, 0.3302, 0.33707, -0.35601, 0.32431, 0.12041, 0.3512, -0.069043, 0.36885, 0.25168, -0.24517, 0.25381, 0.1367, -0.31178, -0.6321, -0.25028, -0.38097], [0.64756, 0.16, 0.029191, 0.35118, 0.089119, 0.61115, -0.66362, -0.51724, -0.46521, -0.08845, 0.0502, 0.26329, 0.12407, 0.043832, 0.17283, 0.01317, 0.14168, -0.15827, -0.10427, -0.9307, 0.21646, -0.10753, 0.62087, 0.36761, -0.48144, -1.28, -0.55152, -0.72023, -0.17097, -0.47993, 4.0165, 0.47054, 0.093614, -0.86341, 0.50881, 0.33353, -0.35962, -0.16648, -0.31803, 0.49003, -0.36697, 0.32051, 0.70932, 0.62878, 0.70128, 0.1302, -0.73769, 0.10325, -0.30964, -0.44213], [0.56447, 0.99891, -0.10346, -0.62079, 0.84667, 0.13505, -1.4362, 0.49886, -0.6645, -0.099902, -0.54387, 0.50594, 0.0085205, -0.041121, 0.74228, -0.19403, -0.077764, 0.21345, 0.4045, -0.41583, 0.30789, 1.4237, 0.41258, 0.80008, 0.66945, -1.4126, -0.49322, -0.49813, 0.073899, -0.86317, 2.3199, 0.85326, -0.21638, -0.52742, -0.098297, 0.15082, -0.53369, -0.22511, -0.061727, -0.31506, -0.040504, 0.17529, -0.33621, 0.36171, 0.84428, -0.30826, -0.73907, -0.91513, -0.48072, 0.22964], [0.15164, 0.30177, -0.16763, 0.17684, 0.31719, 0.33973, -0.43478, -0.31086, -0.44999, -0.29486, 0.16608, 0.11963, -0.41328, -0.42353, 0.59868, 0.28825, -0.11547, -0.041848, -0.67989, -0.25063, 0.18472, 0.086876, 0.46582, 0.015035, 0.043474, -1.4671, -0.30384, -0.023441, 0.30589, -0.21785, 3.746, 0.0042284, -0.18436, -0.46209, 0.098329, -0.11907, 0.23919, 0.1161, 0.41705, 0.056763, -6.3681e-05, 0.068987, 0.087939, -0.10285, -0.13931, 0.22314, -0.080803, -0.35652, 0.016413, 0.10216], [0.27062, -0.36596, 0.097193, -0.50708, 0.37375, 0.16736, -0.94185, 0.54004, -0.66669, -0.24236, 0.25876, 0.28084, -0.86643, -0.068961, 0.90346, 0.40877, -0.39563, -0.25604, -1.0316, -0.26669, -0.080584, 0.40841, 0.55885, -0.18299, 0.46494, -2.2671, 0.14102, 0.19841, 0.5153, -0.27608, 3.3604, 0.15123, -0.36693, -0.28804, 0.076042, -0.076662, 0.21897, 0.39001, 0.38684, -0.16961, -0.33674, 0.37094, -0.45911, 0.00066285, -0.17797, 0.12467, -0.015418, -0.75256, -0.17335, -0.22587], [0.11891, 0.15255, -0.082073, -0.74144, 0.75917, -0.48328, -0.31009, 0.51476, -0.98708, 0.00061757, -0.15043, 0.8377, -1.0797, -0.5146, 1.3188, 0.62007, 0.13779, 0.47108, -0.072874, -0.72675, -0.74116, 0.75263, 0.8818, 0.29561, 1.3548, -2.5701, -1.3523, 0.4588, 1.0068, -1.1856, 3.4737, 0.77898, -0.72929, 0.25102, -0.26156, -0.34684, 0.55841, 0.75098, 0.4983, -0.26823, -0.0027443, -0.018298, -0.28096, 0.55318, 0.037706, 0.18555, -0.15025, -0.57512, -0.26671, 0.92121], [-0.0044021, -0.34141, -0.22729, -0.69283, 0.80244, -0.21823, -0.19036, 0.86436, -0.6988, 0.58897, 0.22517, 0.1399, -0.18051, 0.14046, 0.55502, 0.59184, 0.51439, 0.30326, 0.37276, -1.0549, -1.0332, 1.2613, 0.20113, 0.50856, 1.0459, -1.509, -1.1532, 0.63151, 1.1575, -0.52315, 3.1025, 1.0674, 0.47125, -0.52927, -0.51614, -0.31387, 0.011424, 0.017461, 0.026483, -0.81318, -0.42375, -0.23732, 0.24397, 0.74946, 0.056762, 0.26774, -0.82833, -0.25263, -0.29477, 0.611], [0.27724, 0.88469, -0.26247, 0.084104, 0.40813, -1.1697, -0.68522, 0.1427, -0.57345, -0.58575, -0.50834, -0.86411, -0.52596, -0.56379, 0.32862, 0.43393, -0.21248, 0.49365, -1.8137, -0.035741, 1.3227, 0.80865, 0.012217, -0.087017, -0.16813, -1.5935, 0.47034, 0.26097, -0.41666, -0.38526, 3.4413, 0.34383, -0.035895, -0.5678, 0.18377, -0.48647, 0.42646, 0.4408, 1.0931, 0.063915, -0.064305, -0.29231, 0.086502, 0.35245, 0.17891, 0.25941, 0.37069, -0.51611, 0.023163, 0.05779], [0.57799, 0.99986, -0.53279, -0.50667, 0.78591, -0.12575, -0.22159, 0.3003, -0.068389, -0.15202, -0.23185, -0.30412, -1.2577, 0.013074, -0.01122, 0.37426, 1.3629, -0.64067, 1.1377, 0.3574, 0.13101, 1.2354, -0.18961, -0.76785, 0.43404, -1.0692, 0.13794, -0.29264, 0.24017, 0.48798, 3.3225, 0.11328, -2.4298, -0.26524, -0.41547, -0.63322, -0.2971, -0.74152, -0.78166, 0.21767, -0.97039, -1.0924, -0.37907, -1.4855, -0.080289, 0.31801, -0.38521, 0.74582, -0.95574, -0.92374], [0.25616, 0.43694, -0.11889, 0.20345, 0.41959, 0.85863, -0.60344, -0.31835, -0.6718, 0.003984, -0.075159, 0.11043, -0.73534, 0.27436, 0.054015, -0.23828, -0.13767, 0.011573, -0.46623, -0.55233, 0.083317, 0.55938, 0.51903, -0.27065, -0.28211, -1.3918, 0.17498, 0.26586, 0.061449, -0.273, 3.9032, 0.38169, -0.056009, -0.004425, 0.24033, 0.30675, -0.12638, 0.33436, 0.075485, -0.036218, 0.13691, 0.37762, -0.12159, -0.13808, 0.19505, 0.22793, -0.17304, -0.07573, -0.25868, -0.39339], [-0.24606, -0.36208, 0.16298, -1.0539, 1.015, -0.12986, 0.029943, 0.70045, -0.51904, 0.37297, -0.17724, 0.58895, -0.86737, -0.039516, 0.94961, 0.42915, 0.67331, 0.20087, 0.37476, -0.8886, 0.0036479, 1.1954, 0.70964, 0.39676, 1.5242, -1.5306, -0.62682, 0.14013, 1.0916, -1.1345, 2.2712, 0.9591, -0.40827, -0.247, -0.54859, -0.014245, 0.35309, 0.67128, 0.18092, -0.6552, 0.058029, -0.64046, -0.030244, 0.6986, 0.19883, -0.16348, -0.14961, -0.40892, 0.24814, 0.52553], [0.26818, 0.14346, -0.27877, 0.016257, 0.11384, 0.69923, -0.51332, -0.47368, -0.33075, -0.13834, 0.2702, 0.30938, -0.45012, -0.4127, -0.09932, 0.038085, 0.029749, 0.10076, -0.25058, -0.51818, 0.34558, 0.44922, 0.48791, -0.080866, -0.10121, -1.3777, -0.10866, -0.23201, 0.012839, -0.46508, 3.8463, 0.31362, 0.13643, -0.52244, 0.3302, 0.33707, -0.35601, 0.32431, 0.12041, 0.3512, -0.069043, 0.36885, 0.25168, -0.24517, 0.25381, 0.1367, -0.31178, -0.6321, -0.25028, -0.38097], [0.352, 0.25323, -0.097659, 0.26108, 0.12976, 0.33684, -0.73076, -0.42641, -0.22795, -0.083619, 0.52963, 0.34644, -0.32824, -0.28667, 0.24876, 0.22053, 0.019356, -0.015447, -0.18319, -0.29729, 0.11739, -0.071214, 0.41086, 0.013912, -0.17424, -1.5839, -0.051961, -0.18115, -0.76375, -0.17817, 3.749, -0.045559, 0.10721, -0.51313, 0.25279, -0.051714, 0.31911, 0.28, -0.19937, 0.17819, 0.018623, 0.47641, -0.15655, -0.38287, 0.26989, -0.011186, -0.7244, 0.036514, -0.011489, -0.025882], [0.35663, 0.61973, -0.71631, 0.20023, 0.2629, 0.022202, -0.25861, 0.26357, 0.2839, 0.41427, 0.040724, 0.13203, -0.29023, -0.20172, 0.33643, -0.29397, 0.0095604, 0.56228, -0.11364, -0.44548, -0.28563, 1.0147, -0.21767, 0.056146, 0.96655, -1.0414, -0.98358, -0.59189, 0.60383, 0.56588, 3.1509, 0.86628, 0.17036, -1.3524, 0.080502, 0.51724, -0.6192, 0.65536, -0.063525, -0.46372, -0.3681, -0.10436, -0.087543, 0.65907, -0.11919, 0.10949, 0.41743, 0.29235, -0.37246, 0.76543], [0.21705, 0.46515, -0.46757, 0.10082, 1.0135, 0.74845, -0.53104, -0.26256, 0.16812, 0.13182, -0.24909, -0.44185, -0.21739, 0.51004, 0.13448, -0.43141, -0.03123, 0.20674, -0.78138, -0.20148, -0.097401, 0.16088, -0.61836, -0.18504, -0.12461, -2.2526, -0.22321, 0.5043, 0.32257, 0.15313, 3.9636, -0.71365, -0.67012, 0.28388, 0.21738, 0.14433, 0.25926, 0.23434, 0.4274, -0.44451, 0.13813, 0.36973, -0.64289, 0.024142, -0.039315, -0.26037, 0.12017, -0.043782, 0.41013, 0.1796], [0.39621, 0.15996, 0.46712, 0.18459, 1.1826, -0.29844, -0.44587, 0.13879, -0.79038, -0.21838, -0.33809, -0.68331, -0.37921, -0.11752, 0.042039, 0.48018, -0.012653, -0.45736, -0.77084, -0.047438, 0.31252, 0.5868, 0.11256, -0.25544, 0.13935, -1.3885, 0.66674, 0.28422, 0.14652, 0.21721, 3.2838, -0.064245, 0.087545, 0.059236, 0.18729, -0.7151, -0.45676, 0.42607, -0.075389, -0.21992, -0.12417, 0.12843, 0.33174, 0.089882, -0.15063, -0.08294, -0.066774, -0.19275, 0.24453, -0.28937], [0.66391, 0.57138, 0.019764, 0.427, 0.2381, 0.086552, -0.42532, 0.4828, 0.21486, -0.71015, 0.70173, 0.45035, 0.16789, 0.27565, 0.16198, 0.46422, -0.74168, -0.84952, 0.055707, -0.25098, 0.029031, 0.78097, 0.73322, 0.65038, 0.24298, -1.2368, 0.57383, -0.37277, 0.58804, 0.020214, 2.6541, 0.096423, -0.54564, -0.60966, -0.25546, -0.14909, -0.23799, -0.23651, 0.16118, 0.52908, 0.90116, 0.3068, 0.64582, 0.60057, 0.62972, -0.29534, 0.26915, 0.094594, -0.026029, 0.18051], [0.25616, 0.43694, -0.11889, 0.20345, 0.41959, 0.85863, -0.60344, -0.31835, -0.6718, 0.003984, -0.075159, 0.11043, -0.73534, 0.27436, 0.054015, -0.23828, -0.13767, 0.011573, -0.46623, -0.55233, 0.083317, 0.55938, 0.51903, -0.27065, -0.28211, -1.3918, 0.17498, 0.26586, 0.061449, -0.273, 3.9032, 0.38169, -0.056009, -0.004425, 0.24033, 0.30675, -0.12638, 0.33436, 0.075485, -0.036218, 0.13691, 0.37762, -0.12159, -0.13808, 0.19505, 0.22793, -0.17304, -0.07573, -0.25868, -0.39339], [0.95281, -0.20608, 0.55618, -0.46323, 0.73354, 0.029137, -0.19367, -0.090066, -0.22958, -0.19058, -0.34857, -1.0231, 0.743, -0.5489, 0.88484, -0.14051, 0.0040139, 0.58448, 0.10767, -0.44657, -0.43205, 0.9868, 0.78288, 0.51513, 0.85788, -1.7713, -0.88259, -0.59728, 0.084934, -0.48112, 3.9678, 0.8893, -0.27064, -0.44094, -0.26213, 0.085597, 0.022099, -0.58376, 0.10908, 0.77973, -0.95447, 0.40482, 0.8941, 0.65251, 0.39858, 0.20884, -1.3281, -0.10882, -0.22822, -0.46303], [0.42277, 0.29808, 0.47075, -0.89967, 0.87502, 0.70361, -0.9493, 0.56447, -1.0811, -0.14366, 0.060306, 0.34499, -0.019897, -0.57614, 0.82924, -0.30208, 0.13763, 0.068363, 0.11608, -0.044059, -0.57823, 0.85138, 0.66774, 0.39661, 0.22421, -1.7769, -0.29087, -1.0029, -0.52493, -0.23671, 2.0893, 0.11614, 0.0098968, -0.68153, 0.51195, -0.044244, -0.232, 0.095472, -0.29722, -0.1082, -0.086958, 0.7218, 0.2823, 0.066235, 0.13381, -0.45599, -1.3809, -0.74528, -0.5934, -0.043672], [0.11891, 0.15255, -0.082073, -0.74144, 0.75917, -0.48328, -0.31009, 0.51476, -0.98708, 0.00061757, -0.15043, 0.8377, -1.0797, -0.5146, 1.3188, 0.62007, 0.13779, 0.47108, -0.072874, -0.72675, -0.74116, 0.75263, 0.8818, 0.29561, 1.3548, -2.5701, -1.3523, 0.4588, 1.0068, -1.1856, 3.4737, 0.77898, -0.72929, 0.25102, -0.26156, -0.34684, 0.55841, 0.75098, 0.4983, -0.26823, -0.0027443, -0.018298, -0.28096, 0.55318, 0.037706, 0.18555, -0.15025, -0.57512, -0.26671, 0.92121], [0.10097, 0.15581, -0.65314, -0.57293, 0.46175, -0.12586, 0.44133, -0.026842, -0.17228, 0.36553, 0.27058, 0.30071, 0.080383, -0.481, 0.1809, 0.38219, 0.77298, -0.40468, 0.59932, -0.27045, -0.13876, 0.1615, 0.076312, -0.019777, 0.46294, -1.2998, -0.88866, -0.36823, 0.089705, 0.17793, 2.1316, 0.34826, -0.20278, -1.0421, -0.19542, -0.65008, -0.11556, -0.0085439, -1.0751, 0.055863, -0.31347, -0.36825, 0.47899, 0.46545, 0.02856, -0.21961, -0.60033, 1.0269, 0.034923, -0.018717], [0.31741, -0.23607, -0.24564, -0.8125, -0.16687, 0.084277, 0.15258, -0.026107, 0.19938, 0.22685, 1.2027, 0.47059, 0.034372, -1.0592, -0.34728, 0.61989, -0.25537, -0.05685, 0.65551, -0.39869, -0.19688, -0.44139, -0.058184, -0.79506, 1.0586, -0.9981, -0.57596, -0.067628, 0.2759, 0.63861, 2.6176, 0.57173, 1.0591, -1.2786, -0.21104, -0.23091, -0.24408, 0.89464, -1.1395, 0.12809, -0.55785, -0.34447, 0.32694, -0.23165, -0.25999, -0.14919, -0.43647, -0.035553, -0.43757, 0.28469], [0.15164, 0.30177, -0.16763, 0.17684, 0.31719, 0.33973, -0.43478, -0.31086, -0.44999, -0.29486, 0.16608, 0.11963, -0.41328, -0.42353, 0.59868, 0.28825, -0.11547, -0.041848, -0.67989, -0.25063, 0.18472, 0.086876, 0.46582, 0.015035, 0.043474, -1.4671, -0.30384, -0.023441, 0.30589, -0.21785, 3.746, 0.0042284, -0.18436, -0.46209, 0.098329, -0.11907, 0.23919, 0.1161, 0.41705, 0.056763, -6.3681e-05, 0.068987, 0.087939, -0.10285, -0.13931, 0.22314, -0.080803, -0.35652, 0.016413, 0.10216], [-0.6297, 0.69044, 0.55243, -0.062592, 0.45604, -0.10409, 0.60796, -0.52887, -0.86095, 0.042727, 0.06964, 0.65669, -0.22589, -0.62967, 0.32793, -0.015011, 0.14491, 0.18763, -0.23258, 0.25128, -0.58434, -0.20308, 1.1255, 0.41607, 0.49414, -0.43411, -0.34465, -0.15613, 0.71738, -0.665, 2.102, 0.044444, -0.24947, -0.11691, -0.40116, -0.2891, 0.78036, -0.53631, 0.51513, 0.12626, 0.64765, -0.57939, -0.16513, 0.023644, -0.52152, 0.62582, 0.15187, 0.072398, 0.25744, 1.0729], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpfJdtiitSfq",
        "outputId": "5af4bbcd-26b8-4a31-bb6f-27977d2dc3b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X=np.array(embedded_feel_arr)\n",
        "print(np.shape(X))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7575, 100, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2xrBbIguJTD",
        "outputId": "1ca48f1d-cd94-430d-c1b2-aad9b489d117",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "onehotencoder = OneHotEncoder()\n",
        "Y = onehotencoder.fit_transform(np.array(df[0]).reshape(-1,1)).toarray()\n",
        "print(np.shape(Y))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7575, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0PMftc3u5Vy",
        "outputId": "dd8ef64d-8bea-45ce-d6e6-0c303ca58f84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7575, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym18_cO2x_5b"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C0Jq5za1313"
      },
      "source": [
        "def model1(X,Y,input_size1,input_size2,output_size):\n",
        "  m=Sequential()\n",
        "  m.add(Bidirectional(LSTM(50,input_shape=(input_size1,input_size2))))\n",
        "  m.add(Dropout(0.2))\n",
        "  m.add(Dense(50,activation='relu'))\n",
        "  m.add(Dropout(0.2))\n",
        "  m.add(Dense(output_size,activation='softmax'))\n",
        "  m.compile('Adam','categorical_crossentropy',['accuracy'])\n",
        "  m.fit(X,Y,epochs=10)\n",
        "  return m"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPQQpEXHOn5w",
        "outputId": "b90638a3-9a6d-483a-e3f4-af2f60ec1507",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "m=model1(X_train,Y_train,100,50,7)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "166/166 [==============================] - 11s 64ms/step - loss: 1.8780 - accuracy: 0.2348\n",
            "Epoch 2/10\n",
            "166/166 [==============================] - 11s 63ms/step - loss: 1.6910 - accuracy: 0.3482\n",
            "Epoch 3/10\n",
            "166/166 [==============================] - 11s 64ms/step - loss: 1.6177 - accuracy: 0.3799\n",
            "Epoch 4/10\n",
            "166/166 [==============================] - 11s 64ms/step - loss: 1.5426 - accuracy: 0.4180\n",
            "Epoch 5/10\n",
            "166/166 [==============================] - 11s 64ms/step - loss: 1.5007 - accuracy: 0.4340\n",
            "Epoch 6/10\n",
            "166/166 [==============================] - 11s 64ms/step - loss: 1.4576 - accuracy: 0.4462\n",
            "Epoch 7/10\n",
            "166/166 [==============================] - 11s 65ms/step - loss: 1.4125 - accuracy: 0.4751\n",
            "Epoch 8/10\n",
            "166/166 [==============================] - 11s 64ms/step - loss: 1.3752 - accuracy: 0.4857\n",
            "Epoch 9/10\n",
            "166/166 [==============================] - 11s 64ms/step - loss: 1.3240 - accuracy: 0.5147\n",
            "Epoch 10/10\n",
            "166/166 [==============================] - 11s 64ms/step - loss: 1.2784 - accuracy: 0.5253\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}